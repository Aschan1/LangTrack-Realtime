{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442cd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Point to your environment's Library\\bin (where FFmpeg DLLs live)\n",
    "# Replace 'whisper4' with your actual new environment name if different\n",
    "env_bin_path = r\"C:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Library\\bin\"\n",
    "if os.path.exists(env_bin_path):\n",
    "    os.add_dll_directory(env_bin_path)\n",
    "\n",
    "# 2. Point to the torchcodec folder itself\n",
    "codec_path = r\"C:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages\\torchcodec\"\n",
    "if os.path.exists(codec_path):\n",
    "    os.add_dll_directory(codec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a275814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: accelerate in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: datasets[audio] in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (4.6.0)\n",
      "Collecting datasets[audio]\n",
      "  Using cached datasets-4.6.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (2026.2.19)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock>=3.10.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: typer in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.2.25)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (2.32.5)\n",
      "Requirement already satisfied: xxhash in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (0.70.18)\n",
      "Requirement already satisfied: torchcodec>=0.6.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (0.8.1)\n",
      "Requirement already satisfied: torch>=2.8.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from datasets[audio]) (2.9.1+cu130)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (3.13.3)\n",
      "Requirement already satisfied: psutil in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2026.2.0,>=2023.1.0->datasets[audio]) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from requests>=2.32.2->datasets[audio]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from requests>=2.32.2->datasets[audio]) (2.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from torch>=2.8.0->datasets[audio]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from torch>=2.8.0->datasets[audio]) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from torch>=2.8.0->datasets[audio]) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from torch>=2.8.0->datasets[audio]) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.5,>=1.1.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from sympy>=1.13.3->torch>=2.8.0->datasets[audio]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from jinja2->torch>=2.8.0->datasets[audio]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from pandas->datasets[audio]) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.17.0)\n",
      "Requirement already satisfied: click>=8.2.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from typer->huggingface-hub<2.0,>=1.3.0->transformers) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from typer->huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: rich>=12.3.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from typer->huggingface-hub<2.0,>=1.3.0->transformers) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from typer->huggingface-hub<2.0,>=1.3.0->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from rich>=12.3.0->typer->huggingface-hub<2.0,>=1.3.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from rich>=12.3.0->typer->huggingface-hub<2.0,>=1.3.0->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer->huggingface-hub<2.0,>=1.3.0->transformers) (0.1.2)\n",
      "Using cached datasets-4.6.1-py3-none-any.whl (520 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.6.0\n",
      "    Uninstalling datasets-4.6.0:\n",
      "      Successfully uninstalled datasets-4.6.0\n",
      "Successfully installed datasets-4.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#%pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c641dbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcodec==0.8.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper4\\Lib\\site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Add this to downgrade torchcodec to 0.8.1\n",
    "%pip install torchcodec==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938f7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: cffi in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from sounddevice) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from cffi->sounddevice) (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from soundfile) (2.4.2)\n",
      "Requirement already satisfied: pycparser in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from cffi>=1.0->soundfile) (3.0)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting silero-vad\n",
      "  Using cached silero_vad-6.2.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from silero-vad) (25.0)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from silero-vad) (2.9.1+cu130)\n",
      "Requirement already satisfied: torchaudio>=0.12.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from silero-vad) (2.9.1+cu130)\n",
      "Requirement already satisfied: filelock in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from sympy>=1.13.3->torch>=1.12.0->silero-vad) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from jinja2->torch>=1.12.0->silero-vad) (3.0.2)\n",
      "Using cached silero_vad-6.2.1-py3-none-any.whl (9.1 MB)\n",
      "Installing collected packages: silero-vad\n",
      "Successfully installed silero-vad-6.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sounddevice\n",
    "%pip install soundfile\n",
    "#Install the silero VAD model for voice activity detection\n",
    "%pip install silero-vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994282f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (1.17.1)\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages (from scipy) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2113a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import queue\n",
    "import threading\n",
    "from scipy.signal import resample_poly\n",
    "from math import gcd\n",
    "import soundfile as so\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7610b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as so\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dll_path = Path(r\"C:\\Users\\pavlo\\miniconda3\\Library\\bin\")\n",
    "\n",
    "if dll_path.exists():\n",
    "    os.add_dll_directory(str(dll_path))\n",
    "else:\n",
    "    print(f\"Warning: {dll_path} not found. Check your Miniconda path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db05ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavlo\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pavlo\\.cache\\huggingface\\hub\\models--openai--whisper-tiny. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 578.70it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Latins' work is really Greek after all, and can discover in it but little of Rocky Ithaca. Lennils, pictures, are a sort of upguards and atom paintings, and Mason's exquisite itals are as national as a jingo poem. Mr. Birkut Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampoo and a turkish bath. Next man.\n"
     ]
    }
   ],
   "source": [
    "# Trying out Whisper tiny from: https://huggingface.co/openai/whisper-large-v3\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "device_for_pipeline = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device_for_pipeline,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\",streaming=True)\n",
    "sample = next(iter(dataset))\n",
    "audio_sample=sample[\"audio\"]\n",
    "if isinstance(audio_sample,dict) and \"array\" in audio_sample:\n",
    "    audio_input = audio_sample[\"array\"]\n",
    "elif isinstance(audio_sample,dict) and \"path\" in audio_sample:\n",
    "    audio_input = so.read(audio_sample[\"path\"])\n",
    "else:\n",
    "    audio_input = audio_sample\n",
    "result = pipe(audio_input,return_timestamps=True)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cca2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: c:\\Users\\pavlo\\miniconda3\\envs\\whisper5\\python.exe\n",
      "torch: 2.9.1+cu130\n",
      "CUDA_VISIBLE_DEVICES: None\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, os\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895ef24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n"
     ]
    }
   ],
   "source": [
    "fc = torch.version.cuda\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f173837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 339.18it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Using cache found in C:\\Users\\pavlo/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time transcription. Press Ctrl+C to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech segment lasting 1.4s complete. Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transcript]:  with the staff in the student rest. I'm letting my answer switch on right now. I know.\n",
      "Speech segment lasting 0.3s complete. Transcribing...\n",
      "[Transcript]:  austerity.\n",
      "Speech segment lasting 0.4s complete. Transcribing...\n",
      "[Transcript]:  This is austerity.\n",
      "Speech segment lasting 0.6s complete. Transcribing...\n",
      "[Transcript]:  I know, I know, I'm here.\n",
      "Speech segment lasting 0.7s complete. Transcribing...\n",
      "[Transcript]:  This is so old. Where are we going now?\n",
      "Speech segment lasting 0.5s complete. Transcribing...\n",
      "[Transcript]:  want to define me this box.\n",
      "Speech segment lasting 0.7s complete. Transcribing...\n",
      "[Transcript]:  Please find this red pillow.\n",
      "Speech segment lasting 0.6s complete. Transcribing...\n",
      "[Transcript]:  where my shoes in the scene.\n",
      "Speech segment lasting 0.1s complete. Transcribing...\n",
      "[Transcript]:  Thank you.\n",
      "Speech segment lasting 0.3s complete. Transcribing...\n",
      "[Transcript]:  I don't need that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech segment lasting 0.5s complete. Transcribing...\n",
      "[Transcript]:  I will need some point.\n",
      "Speech segment lasting 0.3s complete. Transcribing...\n",
      "[Transcript]:  He's bare with me.\n",
      "Speech segment lasting 0.6s complete. Transcribing...\n",
      "[Transcript]:  Please bear with me.\n",
      "Speech segment lasting 0.3s complete. Transcribing...\n",
      "[Transcript]:  I will help you.\n",
      "Stopping real-time transcription...\n"
     ]
    }
   ],
   "source": [
    "mf_s=44100 #Standard microphone sampling rate\n",
    "tf_s=16000 #Silero required sampling rate; https://github.com/snakers4/silero-vad/wiki/Performance-Metrics#silero-vad-performance-metrics\n",
    "chunk_size = 32.0 #ms (nominal chunk size for silero (512 chunks at 16kHz); https://github.com/snakers4/silero-vad/wiki/Performance-Metrics#silero-vad-performance-metrics)\n",
    "nsm=int(mf_s*chunk_size/1000)#Number of samples in each chunk with the microphone sampling rate (1323 samples at 44.1kHz)\n",
    "nst=int(tf_s*chunk_size/1000)#Number of samples in each chunk with the target sampling rate (480 samples at 16kHz)\n",
    "min_speech_duration = 0.25 # 250ms minimum speech to be valid\n",
    "VAD_threshold = 0.5 #Silero-VAD's output is a probability of voice presence in the current chunk. The probability value is thresholded to determine a speech classification (https://github.com/snakers4/silero-vad/wiki/Quality-Metrics#probability).\n",
    "st=2.0#Silence time threshold in seconds; if the VAD detects silence for this amount of time, it will consider the speech segment to be complete\n",
    "maxd=10.0#Maximum duration of a speech segment in seconds; if the VAD detects speech for this amount of time, it will consider the speech segment to be complete and buffer will be flushed\n",
    "\n",
    "#Load models\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)    \n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "#device_for_pipeline = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "#Initialize VAD https://github.com/snakers4/silero-vad\n",
    "torch.set_num_threads(1)\n",
    "vad, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "vad.eval()#Set the VAD to evaluation mode\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils#Extract the utility functions from the silero VAD utils https://medium.com/@aidenkoh/how-to-implement-high-speed-voice-recognition-in-chatbot-systems-with-whisperx-silero-vad-cdd45ea30904\n",
    "# Initialize VADIterator\n",
    "vad_iterator = VADIterator(vad, sampling_rate=tf_s, threshold=VAD_threshold)\n",
    "\n",
    "aq=queue.Queue()#Audio queue for communication between the audio callback and the main thread\n",
    "sbuf=[]#Speech buffer to store the current speech segment\n",
    "scount=0#Silence counter\n",
    "speaking=False#Flag to indicate if the user is currently speaking\n",
    "\n",
    "sb=int(st*(1000/chunk_size))#Number of consecutive silent chunks required to consider the speech segment complete\n",
    "spb=int(maxd*(1000/chunk_size))#Number of consecutive speech chunks required to consider the speech segment complete\n",
    "min_spb = int(min_speech_duration * (1000 / chunk_size)) # Minimum chunks required for chunk to be considered valid speech segment\n",
    "\n",
    "#Function to add audio samples to queue\n",
    "def inq(insamples, frames, time_info, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    aq.put(insamples)#Add the incoming audio samples to the queue (queue method: https://www.geeksforgeeks.org/python/queue-in-python/)\n",
    "#Function to resample audio samples from the microphone sampling rate to the target sampling rate required by the VAD model\n",
    "def resample(inaudio):\n",
    "    seq=inaudio.flatten().astype(np.float32) #Get the audio samples from the queue\n",
    "    #Resampling is input f*(tf_s/mf_s), i.e. mf_s*(tf_s/mf_s)=tf_s=16KHz. Better than using scipy.signal.resample, which adds samples and might introduce artifacts or aliasing effects. This allows for more control over frequencies. \n",
    "    resampled = resample_poly(seq, tf_s, mf_s)#Resample the audio samples to the target sampling rate using polyphase filtering (up then down conversion with appropriate anti-aliasing filtering after down conversion) (scipy method: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample_poly.html)\n",
    "    return resampled.astype('float32')#Return the resampled audio samples as float32 (required by silero VAD; https://medium.com/@aidenkoh/how-to-implement-high-speed-voice-recognition-in-chatbot-systems-with-whisperx-silero-vad-cdd45ea30904)\n",
    "\n",
    "#Silero-VAD inference function\n",
    "def VAD_prob(sample_16k):\n",
    "    with torch.no_grad():#Disable gradient calculation for inference\n",
    "        audio_tensor = torch.from_numpy(sample_16k).float().unsqueeze(0)#Convert the resampled audio samples to a PyTorch tensor and add a batch dimension\n",
    "        #prob = vad(audio_tensor, tf_s).item()#Get the VAD probability of voice presence in the current chunk (sampling rate not passed as keyword argument detected after error:Error in process_audio: RuntimeError: forward() is missing value for argument 'sr')\n",
    "        vad_iterator(audio_tensor, return_seconds=True)\n",
    "    return 1.0 if vad_iterator.triggered else 0.0#Detect if threshold is triggered and return 1.0 for speech and 0.0 for silence based on the VAD iterator's triggered state \n",
    "\n",
    "#Whisper inference function\n",
    "def transcribe(speech_Segment):\n",
    "    audio_a=np.concatenate(speech_Segment, axis=0)#Concatenate the audio chunks in the speech buffer to form a complete speech segment\n",
    "    audio_a=audio_a/(np.max(np.abs(audio_a))+1e-6)#Normalize the audio samples to the range [-1, 1] to prevent clipping and ensure consistent volume levels \n",
    "    result = pipe(audio_a)#Transcribe the speech segment using the Whisper model and get the transcription result with timestamps\n",
    "    #print(result[\"text\"])#Print the transcribed text from the speech segment\n",
    "    return result[\"text\"]#Return the transcribed text from the speech segment\n",
    "    \n",
    "#Process audio chunks from the queue and perform VAD and transcription\n",
    "def process_audio():\n",
    "    global scount, speaking, sbuf\n",
    "    while True:\n",
    "        audio = aq.get()#Get the next audio chunk from the queue\n",
    "        if audio is None:#If the audio chunk is None, it means the audio stream has ended, so the loop is exited\n",
    "            break\n",
    "        resampled_16=resample(audio)#Resample the audio chunk to the target sampling rate (16KHz) required by the VAD model\n",
    "        if len(resampled_16) < nst:#If the resampled audio chunk is shorter than the expected number of samples for the VAD model, it is padded with zeros to ensure consistent input size\n",
    "            resampled_16 = np.pad(resampled_16, (0, nst - len(resampled_16)))\n",
    "        if len(resampled_16) > nst:#If the resampled audio chunk is longer than the expected number of samples for the VAD model, it is truncated to ensure the target number of samples for the VAD\n",
    "            resampled_16 = resampled_16[:nst]\n",
    "        prob = VAD_prob(resampled_16)#Get the VAD probability of voice presence in the current chunk\n",
    "        if prob:#If the VAD probability exceeds the threshold, it is classified as speech\n",
    "            sbuf.append(resampled_16)#The resampled audio chunk is added to the speech buffer\n",
    "            scount=0#The silence counter is set to zero since speech is detected\n",
    "            speaking=True#Set the speaking flag to True since speech is detected\n",
    "        else:#If the VAD probability does not exceed the threshold, it is classified as silence\n",
    "            if speaking: #If the speaker was previously speaking, the silence counter is incremented\n",
    "                scount += 1\n",
    "                if scount >= sb or len(sbuf) >= spb:#If the silence counter exceeds the silence threshold or the speech segments exceed the maximum number of speech segments\n",
    "                    if len(sbuf) >= min_spb:#If the number of speech segments in the buffer exceeds the minimum required for a valid speech segment, it is transcribed\n",
    "                        print(f\"Speech segment lasting {len(sbuf)*(maxd/1000):.1f}s complete. Transcribing...\")\n",
    "                        res = transcribe(sbuf)#The current speech segment in the speech buffer is transcribed using the Whisper model\n",
    "                        sbuf=[]#The speech buffer is cleared to prepare for the next speech segment\n",
    "                        scount=0#The silence counter is reset to zero\n",
    "                        speaking=False#Set the speaking flag to False since the speech segment is complete\n",
    "                        print(f\"[Transcript]: {res}\")#Print the transcribed text from the speech segment\n",
    "                    else:#If the number of speech segments in the buffer does not exceed the minimum required for a valid speech segment, it is discarded\n",
    "                        scount=0#The silence counter is reset to zero\n",
    "                        speaking=False#Set the speaking flag to False since the speech segment is discarded\n",
    "                        print(f\"Discarded speech segment lasting {len(sbuf)*(maxd/1000):.1f}s due to insufficient duration.\")#Print a message indicating that the speech segment was discarded due to insufficient duration\n",
    "                        sbuf=[]#The speech buffer is cleared to discard the current speech segment\n",
    "#Real-time transcription in main\n",
    "def main():\n",
    "    print(\"Starting real-time transcription. Press Ctrl+C to stop.\")\n",
    "    #Threads: https://www.geeksforgeeks.org/python/multithreading-python-set-1/\n",
    "    audio_thread = threading.Thread(target=process_audio)#Create a thread to process the audio chunks from the queue and perform VAD and transcription\n",
    "    audio_thread.start()#Start the audio processing thread\n",
    "    #Exception of keyboard interrupt to stop the audio-streaming and transcription \n",
    "    try:\n",
    "        with sd.InputStream(samplerate=mf_s, channels=1, callback=inq, blocksize=nsm, dtype='float32'):#Start an audio input stream with the specified sampling rate, number of \n",
    "            #channels, callback function, block size, and data type\n",
    "            #https://python-sounddevice.readthedocs.io/en/0.4.1/usage.html\n",
    "            #Explanation of why the main thread needs to be kept alive: The audio stream runs in a separate thread and continuously captures audio data from the microphone.\n",
    "            #If the main thread were to be terminated, the audio stream would also be stopped, and the real-time transcription would not work. By keeping the main thread alive, \n",
    "            # we ensure that the audio stream continues to run and \n",
    "            # captures audio data for processing by the VAD and Whisper models.\n",
    "            while True:\n",
    "                sd.sleep(1000)#Keep the main thread alive while the audio stream is active\n",
    "    except KeyboardInterrupt: #https://www.geeksforgeeks.org/python/how-to-catch-a-keyboardinterrupt-in-python/\n",
    "        print(\"Stopping real-time transcription...\")\n",
    "        aq.put(None)#Signal the audio processing thread to stop by putting None in the queue\n",
    "        audio_thread.join()#Wait for the audio processing thread to finish https://www.geeksforgeeks.org/python/multithreading-python-set-1/\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
