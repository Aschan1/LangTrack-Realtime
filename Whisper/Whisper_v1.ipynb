{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a275814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938f7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: cffi in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from sounddevice) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from cffi->sounddevice) (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soundfile in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from soundfile) (2.4.2)\n",
      "Requirement already satisfied: pycparser in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from cffi>=1.0->soundfile) (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: silero-vad in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.16.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from silero-vad) (1.24.2)\n",
      "Requirement already satisfied: packaging in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from silero-vad) (25.0)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from silero-vad) (2.10.0)\n",
      "Requirement already satisfied: torchaudio>=0.12.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from silero-vad) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from onnxruntime>=1.16.1->silero-vad) (25.12.19)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from onnxruntime>=1.16.1->silero-vad) (2.4.2)\n",
      "Requirement already satisfied: protobuf in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from onnxruntime>=1.16.1->silero-vad) (6.33.5)\n",
      "Requirement already satisfied: sympy in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from onnxruntime>=1.16.1->silero-vad) (1.14.0)\n",
      "Requirement already satisfied: filelock in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (4.15.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from torch>=1.12.0->silero-vad) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from sympy->onnxruntime>=1.16.1->silero-vad) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from jinja2->torch>=1.12.0->silero-vad) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install sounddevice\n",
    "%pip install soundfile\n",
    "#Install the silero VAD model for voice activity detection\n",
    "%pip install silero-vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994282f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (1.17.1)\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages (from scipy) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2113a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import queue\n",
    "import threading\n",
    "from scipy.signal import resample_poly\n",
    "from math import gcd\n",
    "import soundfile as so\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7610b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as so\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dll_path = Path(r\"C:\\Users\\pavlo\\miniconda3\\Library\\bin\")\n",
    "\n",
    "if dll_path.exists():\n",
    "    os.add_dll_directory(str(dll_path))\n",
    "else:\n",
    "    print(f\"Warning: {dll_path} not found. Check your Miniconda path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db05ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavlo\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pavlo\\.cache\\huggingface\\hub\\models--openai--whisper-tiny. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 578.70it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Latins' work is really Greek after all, and can discover in it but little of Rocky Ithaca. Lennils, pictures, are a sort of upguards and atom paintings, and Mason's exquisite itals are as national as a jingo poem. Mr. Birkut Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like a shampoo and a turkish bath. Next man.\n"
     ]
    }
   ],
   "source": [
    "# Trying out Whisper tiny from: https://huggingface.co/openai/whisper-large-v3\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "device_for_pipeline = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device_for_pipeline,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\",streaming=True)\n",
    "sample = next(iter(dataset))\n",
    "audio_sample=sample[\"audio\"]\n",
    "if isinstance(audio_sample,dict) and \"array\" in audio_sample:\n",
    "    audio_input = audio_sample[\"array\"]\n",
    "elif isinstance(audio_sample,dict) and \"path\" in audio_sample:\n",
    "    audio_input = so.read(audio_sample[\"path\"])\n",
    "else:\n",
    "    audio_input = audio_sample\n",
    "result = pipe(audio_input,return_timestamps=True)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cca2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: c:\\Users\\pavlo\\miniconda3\\envs\\whisper\\python.exe\n",
      "torch: 2.10.0+cpu\n",
      "CUDA_VISIBLE_DEVICES: None\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, os\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895ef24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "fc = torch.version.cuda\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f173837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 302.78it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Using cache found in C:\\Users\\pavlo/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time transcription. Press Ctrl+C to stop.\n",
      "Speech segment lasting 0.1s complete. Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transcript]:  Thanks for watching!\n",
      "Stopping real-time transcription...\n"
     ]
    }
   ],
   "source": [
    "mf_s=44100 #Standard microphone sampling rate\n",
    "tf_s=16000 #Silero required sampling rate; https://github.com/snakers4/silero-vad/wiki/Performance-Metrics#silero-vad-performance-metrics\n",
    "chunk_size = 32.0 #ms (nominal chunk size for silero (512 chunks at 16kHz); https://github.com/snakers4/silero-vad/wiki/Performance-Metrics#silero-vad-performance-metrics)\n",
    "nsm=int(mf_s*chunk_size/1000)#Number of samples in each chunk with the microphone sampling rate (1323 samples at 44.1kHz)\n",
    "nst=int(tf_s*chunk_size/1000)#Number of samples in each chunk with the target sampling rate (480 samples at 16kHz)\n",
    "VAD_threshold = 0.5 #Silero-VAD's output is a probability of voice presence in the current chunk. The probability value is thresholded to determine a speech classification (https://github.com/snakers4/silero-vad/wiki/Quality-Metrics#probability).\n",
    "st=2.0#Silence time threshold in seconds; if the VAD detects silence for this amount of time, it will consider the speech segment to be complete\n",
    "maxd=10.0#Maximum duration of a speech segment in seconds; if the VAD detects speech for this amount of time, it will consider the speech segment to be complete and buffer will be flushed\n",
    "\n",
    "#Load models\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)    \n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "#device_for_pipeline = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "#Initialize VAD https://github.com/snakers4/silero-vad\n",
    "torch.set_num_threads(1)\n",
    "vad, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "vad.eval()#Set the VAD to evaluation mode\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils#Extract the utility functions from the silero VAD utils https://medium.com/@aidenkoh/how-to-implement-high-speed-voice-recognition-in-chatbot-systems-with-whisperx-silero-vad-cdd45ea30904\n",
    "# Initialize VADIterator\n",
    "vad_iterator = VADIterator(vad, sampling_rate=tf_s, threshold=VAD_threshold)\n",
    "\n",
    "aq=queue.Queue()#Audio queue for communication between the audio callback and the main thread\n",
    "sbuf=[]#Speech buffer to store the current speech segment\n",
    "scount=0#Silence counter\n",
    "speaking=False#Flag to indicate if the user is currently speaking\n",
    "\n",
    "sb=int(st*(1000/chunk_size))#Number of consecutive silent chunks required to consider the speech segment complete\n",
    "spb=int(maxd*(1000/chunk_size))#Number of consecutive speech chunks required to consider the speech segment complete\n",
    "\n",
    "#Function to add audio samples to queue\n",
    "def inq(insamples, frames, time_info, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    aq.put(insamples)#Add the incoming audio samples to the queue (queue method: https://www.geeksforgeeks.org/python/queue-in-python/)\n",
    "#Function to resample audio samples from the microphone sampling rate to the target sampling rate required by the VAD model\n",
    "def resample(inaudio):\n",
    "    seq=inaudio.flatten().astype(np.float32) #Get the audio samples from the queue\n",
    "    #Resampling is input f*(tf_s/mf_s), i.e. mf_s*(tf_s/mf_s)=tf_s=16KHz. Better than using scipy.signal.resample, which adds samples and might introduce artifacts or aliasing effects. This allows for more control over frequencies. \n",
    "    resampled = resample_poly(seq, tf_s, mf_s)#Resample the audio samples to the target sampling rate using polyphase filtering (up then down conversion with appropriate anti-aliasing filtering after down conversion) (scipy method: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample_poly.html)\n",
    "    return resampled.astype('float32')#Return the resampled audio samples as float32 (required by silero VAD; https://medium.com/@aidenkoh/how-to-implement-high-speed-voice-recognition-in-chatbot-systems-with-whisperx-silero-vad-cdd45ea30904)\n",
    "\n",
    "#Silero-VAD inference function\n",
    "def VAD_prob(sample_16k):\n",
    "    with torch.no_grad():#Disable gradient calculation for inference\n",
    "        audio_tensor = torch.from_numpy(sample_16k).float().unsqueeze(0)#Convert the resampled audio samples to a PyTorch tensor and add a batch dimension\n",
    "        #prob = vad(audio_tensor, tf_s).item()#Get the VAD probability of voice presence in the current chunk (sampling rate not passed as keyword argument detected after error:Error in process_audio: RuntimeError: forward() is missing value for argument 'sr')\n",
    "        vad_iterator(audio_tensor, return_seconds=True)\n",
    "    return 1.0 if vad_iterator.triggered else 0.0#Detect if threshold is triggered and return 1.0 for speech and 0.0 for silence based on the VAD iterator's triggered state \n",
    "\n",
    "#Whisper inference function\n",
    "def transcribe(speech_Segment):\n",
    "    audio_a=np.concatenate(speech_Segment, axis=0)#Concatenate the audio chunks in the speech buffer to form a complete speech segment\n",
    "    audio_a=audio_a/(np.max(np.abs(audio_a))+1e-6)#Normalize the audio samples to the range [-1, 1] to prevent clipping and ensure consistent volume levels \n",
    "    result = pipe(audio_a)#Transcribe the speech segment using the Whisper model and get the transcription result with timestamps\n",
    "    #print(result[\"text\"])#Print the transcribed text from the speech segment\n",
    "    return result[\"text\"]#Return the transcribed text from the speech segment\n",
    "    \n",
    "#Process audio chunks from the queue and perform VAD and transcription\n",
    "def process_audio():\n",
    "    global scount, speaking, sbuf\n",
    "    while True:\n",
    "        audio = aq.get()#Get the next audio chunk from the queue\n",
    "        if audio is None:#If the audio chunk is None, it means the audio stream has ended, so the loop is exited\n",
    "            break\n",
    "        resampled_16=resample(audio)#Resample the audio chunk to the target sampling rate (16KHz) required by the VAD model\n",
    "        if len(resampled_16) < nst:#If the resampled audio chunk is shorter than the expected number of samples for the VAD model, it is padded with zeros to ensure consistent input size\n",
    "            resampled_16 = np.pad(resampled_16, (0, nst - len(resampled_16)))\n",
    "        if len(resampled_16) > nst:#If the resampled audio chunk is longer than the expected number of samples for the VAD model, it is truncated to ensure teh target number of samples for the VAD\n",
    "            resampled_16 = resampled_16[:nst]\n",
    "        prob = VAD_prob(resampled_16)#Get the VAD probability of voice presence in the current chunk\n",
    "        if VAD_prob(resampled_16):#If the VAD probability exceeds the threshold, it is classified as speech\n",
    "            sbuf.append(resampled_16)#The resampled audio chunk is added to the speech buffer\n",
    "            scount=0#The silence counter is set to zero since speech is detected\n",
    "            speaking=True#Set the speaking flag to True since speech is detected\n",
    "        else:#If the VAD probability does not exceed the threshold, it is classified as silence\n",
    "            if speaking: #If the speaker was previously speaking, the silence counter is incremented\n",
    "                scount += 1\n",
    "                if scount >= sb or len(sbuf) >= spb:#If the silence counter exceeds the silence threshold or the speech segments exceed the maximum number of speech segments\n",
    "                    print(f\"Speech segment lasting {len(sbuf)*(maxd/1000):.1f}s complete. Transcribing...\")\n",
    "                    res = transcribe(sbuf)#The current speech segment in the speech buffer is transcribed using the Whisper model\n",
    "                    sbuf=[]#The speech buffer is cleared to prepare for the next speech segment\n",
    "                    scount=0#The silence counter is reset to zero\n",
    "                    speaking=False#Set the speaking flag to False since the speech segment is complete\n",
    "                    print(f\"[Transcript]: {res}\")#Print the transcribed text from the speech segment\n",
    "#Real-time transcription in main\n",
    "def main():\n",
    "    print(\"Starting real-time transcription. Press Ctrl+C to stop.\")\n",
    "    #Threads: https://www.geeksforgeeks.org/python/multithreading-python-set-1/\n",
    "    audio_thread = threading.Thread(target=process_audio)#Create a thread to process the audio chunks from the queue and perform VAD and transcription\n",
    "    audio_thread.start()#Start the audio processing thread\n",
    "    #Exception of keyboard interrupt to stop the audio-streaming and transcription \n",
    "    try:\n",
    "        with sd.InputStream(samplerate=mf_s, channels=1, callback=inq, blocksize=nsm, dtype='float32'):#Start an audio input stream with the specified sampling rate, number of \n",
    "            #channels, callback function, block size, and data type\n",
    "            #https://python-sounddevice.readthedocs.io/en/0.4.1/usage.html\n",
    "            #Explanation of why the main thread needs to be kept alive: The audio stream runs in a separate thread and continuously captures audio data from the microphone.\n",
    "            #If the main thread were to be terminated, the audio stream would also be stopped, and the real-time transcription would not work. By keeping the main thread alive, \n",
    "            # we ensure that the audio stream continues to run and \n",
    "            # captures audio data for processing by the VAD and Whisper models.\n",
    "            while True:\n",
    "                sd.sleep(1000)#Keep the main thread alive while the audio stream is active\n",
    "    except KeyboardInterrupt: #https://www.geeksforgeeks.org/python/how-to-catch-a-keyboardinterrupt-in-python/\n",
    "        print(\"Stopping real-time transcription...\")\n",
    "        aq.put(None)#Signal the audio processing thread to stop by putting None in the queue\n",
    "        audio_thread.join()#Wait for the audio processing thread to finish https://www.geeksforgeeks.org/python/multithreading-python-set-1/\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
